<!DOCTYPE HTML>
<!--
  Based on
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-117339330-4"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-117339330-4');
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <title>
      iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis
    </title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="landing">

		<!-- Banner -->
			<section id="banner" style="background-attachment:scroll;">
        <h2>
          iPOKE: Poking a Still Image for Controlled Stochastic Video Synthesis
        </h2>
        <p>
        <a href="https://www.linkedin.com/in/andreas-blattmann-479038186/?originalSubdomain=de">Andreas Blattmann</a>,
        <a href="hhttps://timomilbich.github.io/">Timo Milbich</a>,
        <a href="https://mdork.github.io/">Michael Dorkenwald</a>,
        <a href="https://hci.iwr.uni-heidelberg.de/Staff/bommer">Bj&ouml;rn Ommer</a><br/>
        <a href="https://www.iwr.uni-heidelberg.de/">Interdisciplinary Center for Scientific Computing, HCI, Heidelberg University</a><br/>

        </p>
                </section>

             <section id="one" class="wrapper style1">

          <div class="container 75%">


<!--        <b>tl;dr </b>-->
<!--              We present <b>iPOKE</b>, a model for locally controlled, stochastic video synthesis by poking a single pixel in a static scene, that enables users to animate still images only with simple mouse drags.-->
<!--        <br>-->
<!--        <br>-->

         <div class="image centered captioned align-just"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/fpp_final.png" alt="" style="border:0px solid black"/>
                      <strong>TL;DR:</strong>
             We present <em>iPOKE</em>, a model for locally controlled, stochastic video synthesis based on poking a single pixel in a static scene, that enables users to animate still images only with simple mouse drags.
                    </div>



            <div class="row 200%">
              <div style="width: 25%">
                              <div class="container 25%">

                    <div class="image fit captioned align-center"
                                style="margin-bottom:0em; box-shadow:0 0">
                      <a href="">
                        <img src="images/paper.png" alt="" style="border:1px solid black"/>
                      </a>
                      <a href="">arXiv</a>
                      <div class="headerDivider"></div>
                      <a href="images/paper.bib">BibTeX</a>
                      <div class="headerDivider"></div>
                      <a href="https://github.com/CompVis/ipoke">GitHub</a>
                      <br/>

                                  </div>
                              </div>
              </div>
              <div style="text-align: justify; margin-left: 10%; width: 65%">
                            <h1>Abstract</h1>
                  <p>How would a static scene react to a local poke? What are the effects on other parts of an object if you could locally push it? There will be distinctive movement, despite evident variations caused by the stochastic nature of our world. These outcomes are governed by the characteristic kinematics of objects that dictate their overall motion caused by a local interaction. Conversely, the movement of an object provides crucial information about its underlying distinctive kinematics and the interdependencies between its parts. This two-way relation motivates learning a bijective mapping between object kinematics and plausible future image sequences. Therefore, we propose iPOKE -- invertible Prediction of Object Kinematics -- that, conditioned on an initial frame and a local poke, allows to sample object kinematics and establishes a one-to-one correspondence to the corresponding plausible videos, thereby providing a controlled stochastic video synthesis. In contrast to previous works, we do not generate arbitrary realistic videos, but provide efficient control of movements, while still capturing the stochastic nature of our environment and the diversity of plausible outcomes it entails. Moreover, our approach can transfer kinematics onto novel object instances and is not confined to particular object classes.
                  </p>
              </div>
            </div>
          </div>




            </section>



                    <section id="three" class="wrapper style2 special">
          <div class="container">
            <header class="major">
              <h2>Enabling Users to Interact with still images</h2>
                <br>
             </header>



<header class="minor">
<h2> We present a Graphical User Interface to directly enable human users to test our model and interact with still images as visualized below.
    <a href="https://github.com/CompVis/ipoke#graphical-user-interface"><p style="color:red">Check it out now!</p></a>
 </h2>
</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/B3-GUI_Demo_2.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results from our GUI. If you also want to check it out, just clone <a href="https://github.com/CompVis/ipoke">our code</a> and follow the instructions there!
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/B3-GUI_Demo_1.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Results from our GUI. If you also want to check it out, just clone <a href="https://github.com/CompVis/ipoke">our code</a> and follow the instructions there!
</div>
</div>
</div>


</div>
</section>

			<!-- Two -->
				<section id="two" class="wrapper style2 special">
					<div class="container">
						<header class="major">
							<h2>Approach</h2>
                            <br><h3>Invertible Model for Controlled Stochastic Video Synthesis</h3>
						</header>

                        <div class="container 75%">
                    <div class="image fit captioned align-left"
                                style="margin-bottom:2em; box-shadow:0 0;
                                text-align:justify">
                      <img src="images/model_figure1.png" alt="" style="border:0px solid black"/>

                    </div>
                            Overview of our proposed framework iPOKE for controlled video synthesis: We apply a conditional bijective transformation \(\tau_{\theta}\) to learn a residual kinematics representation \(r\) capturing all video information not present in the user control \(c\) defining intended local object motion in an image frame \(x_0\) (orange path). To retain feasible computational complexity, we pre-train a video autoencoding framework \((E, GRU, D)\) (blue path) yielding a dedicated video representation \(z\) as training input for \(\tau_\theta\). Controlled video synthesis is achieved by sampling a residual \(r\), thus inferring plausible motion for the remaining object parts not defined in \(c\), and generating video sequences \(\hat{\boldsymbol{X}}\) from the resulting \(z = \tau_{\theta}(r \vert x_0,c)\) using \(GRU\) and \(D\) (black path).
                        </div>
                    </div>
                </section>





<section id="four" class="wrapper style2 special">
<div class="container 85%">
        <header class="major">
            <h2>Results on the Object Classes of Humans and Plants</h2>
        </header>
        <header class="minor">
<h2> Controlled Stochastic Video Synthesis </h2>
</header>

<div class="row 250%">
<div class="6u 12u$(xsmall)">
<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A1.1-Samples_PP.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
    <b>Controlled stochastic video synthesis on PokingPlants [1].</b> Each rows show different samples from our learned residual kinematics distribution for the same poke, indicating our model to be capable of stochastically synthesize motion which can while maintaining localized control.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A1.2-Samples_Iper.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
    <b>Controlled stochastic video synthesis on iPER[2].</b> Each row shows different samples from our learned residual kinematics distribution for the same poke, indicating our model to be capable of stochastically synthesize motion which can while maintaining localized control.
</div>
</div>
</div>


<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A1.3-Samples_Human36m.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
    <b>Controlled stochastic video synthesis on Human3.6m [3].</b> Each row shows different samples from our learned residual kinematics distribution for the same poke, indicating our model to be capable of stochastically synthesize motion which can while maintaining localized control.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A1.4-Samples_TaiChi.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
    <b>Controlled stochastic video synthesis on Tai-Chi-HD [4].</b> Each row shows different samples from our learned residual kinematics distribution for the same poke, indicating our model to be capable of stochastically synthesize motion which can while maintaining localized control.
</div>
</div>
</div>

<header class="minor">
<h2>Further Results</h2>
</header>

<p>
    <b>Kinematics Transfer and Control Sensitivity</b>
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A2-Kinematics_Transfer_Iper.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
    <b>Kinematics Transfer on iPER:</b> We extract the residual kinematics from a ground truth sequence (top row) and use it together with the corresponding control \(c\) (red arrow) to animate an image \(x_t\) showing similar initial object posture (second row). We also visualize a random sample from \(q(r)\) for the same \((x_t,c)\) (bottom row), indicating that the residual kinematics representation solely contains motion information not present in \((x_t,c)\).
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/A3-Control_Sensitivity_Iper_.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
<b>Control Sensitivity:</b> Evaluating the sensitivity of our model to the different poke vectors at the same pixel on iPER [2]. For a given poke location in an image \(x_0\), we randomly sample four poke magnitudes and directions and visualize the resulting synthesized sequences in the rows of this video.
</div>
</div>
</div>

<p>
    <b>Additional vidualized results from real user input</b>
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/B2-Human_Control_Iper.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Comparing different user inputs for the same source image on iPER [2].
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/B1-Human_Control_PP.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Comparing different user inputs for the same source image on PlokingPlants [1].
</div>
</div>
</div>

<header class="minor">
<h2>Understanding Object Structure</h2>
</header>


<div class="row 250%">
<div class="12u$ 12u$(xsmall)">
<div class="image fit captioned align-just">
<!--<a href="images/object_structure.png">-->
<img src="images/body_part.png" alt="" />
    <b>Understanding object structure:</b> By performing 100 random interactions at the same location \(l\) within a given image frame \(x_0\) we obtain varying video sequences, from which we compute motion correlations for \(l\) with all remaining pixels. By mapping these correlations to the pixel space, we visualize distinct object parts.
</div>
</div>
</div>

   <header class="minor">
<h2>Comparison with other models</h2>
</header>
        <p>
            <b>Stochastic Video Synthesis:</b> We compare with the recent state of the art in stochastic video prediction and obtain results obtaining higher visual and temporal fidelity as well as more diversity
</p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/C2.2-Cmp_IVRNN_Iper.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Besides the quantitative results presented in our <a href="">paper</a>, we qualititatively compare iPOKE to IVRNN [5], which is the best performing competing method when considering both diversity and video quality. Here we show the results on iPER [2].
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/C2.1-Cmp_IVRNN_PP.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Besides the quantitative results presented in our <a href="">paper</a>, we qualititatively compare iPOKE to IVRNN [5], which is the best performing competing method when considering both diversity and video quality. Here we show the results on PokingPlants [1].
</div>
</div>
</div>

        <p>
            <b>Controlled Video Synthesis:</b> We compare with the controlled video synthesis baseline of Hao et al. [6] which is the closest related work to our model in controlled video synthesis. iPOKE performs clearly better in terms of controllability and while synthesizing videos with superior visual and temporal coherence . </p>

<div class="row 250%">
<div class="6u 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/C1.2-Cmp_Hao_Iper.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Comparison in controlled video synthesis with the approach of Hao et al. [6] on iPER.
</div>
</div>
<div class="6u$ 12u$(xsmall)">

<div class="image fit captioned align-just">
<div class="videocontainer">
<video controls class="videothing">
<source src="images/C1.1-Cmp_Hao_PP.mp4" type="video/mp4">
Your browser does not support the video tag.
</video>
</div>
Comparison in controlled video synthesis with the approach of Hao et al. [6] on PlokingPlants.
</div>
</div>
</div>

</div>
</section>

        <!-- related works ! -->

<section id="related" class="wrapper style1 special">

<div class="container 75%">
<div class="row 250%">
<div class="12u">
<h2>References </h2>
<div class="12u">
  <p align="justify" style="line-height: 1.0em; font-size:0.8em">
[1] Andreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. Understanding object dynamics for interactive image-to-video synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5171–5181, 2021.
<br/>
[2] Wen Liu, Zhixin Piao, Jie Min, Wenhan Luo, Lin Ma, and Shenghua Gao. Liquid warping gan: A unified framework for human motion imitation, appearance transfer and novel view synthesis. In Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), 2019.
<br/>
[3] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(7): 1325–1339, 2014.
<br/>
[4] Aliaksandr Siarohin, Stéphane Lathuiliére, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Adv. Neural Inform. Process. Syst., pages 7135–7145, 2019.
<br/>
[5] L. Castrejon, N. Ballas, and A. Courville. Improved conditional vrnns for video prediction. In 2019 IEEE/CVF International
Conference on Computer Vision (ICCV), 2019.
<br/>
[6] Zekun Hao, Xun Huang, and Serge Belongie. Controllable video generation with sparse trajectories.  In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
</div>
</div>
</div>
</div>
</section>


<section id="other_work" class="wrapper style1 special">

<div class="container 85%">
<div class="row 250%">
<div class="12u">
<h2> Our Related work on video synthesis</h2>
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/interactive-image2video-synthesis/">
 Understanding Object Dynamics for Interactive Image-To-Video Synthesis
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/interactive-image2video-synthesis/">
<img src="https://compvis.github.io/interactive-image2video-synthesis/images/first-page.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
What would be the effect of locally poking a static scene? We present an approach that learns naturally-looking global articulations caused by a local manipulation at a pixel level. Training requires only videos of moving objects but no information of the underlying manipulation of the physical scene. Our generative model learns to infer natural object dynamics as a response to user interaction and learns about the interrelations between different object body regions. Given a static image of an object and a local poking of a pixel, the approach then predicts how the object would deform over time. In contrast to existing work on video prediction, we do not synthesize arbitrary realistic videos but enable local interactive control of the deformation. Our model is not restricted to particular object categories and can transfer dynamics onto novel unseen object instances. Extensive experiments on diverse objects demonstrate the effectiveness of our approach compared to common video prediction frameworks.
</div>


<div class="12u">
<h6>
<a href="https://github.com/CompVis/image2video-synthesis-using-cINNs">
 Stochastic Image-To-Video Synthesis using cINNs
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/image2video-synthesis-using-cINNs/">
<img src="https://compvis.github.io/image2video-synthesis-using-cINNs/paper/method.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Video understanding calls for a model to learn the characteristic interplay between static scene content and its dynamics: Given an image, the model must be able to predict a future progression of the portrayed scene and, conversely, a video should be explained in terms of its static image content and all the remaining characteristics not present in the initial frame. This naturally suggests a bijective mapping between the video domain and the static content as well as residual information. In contrast to common stochastic image-to-video synthesis, such a model does not merely generate arbitrary videos progressing the initial image. Given this image, it rather provides a one-to-one mapping between the residual vectors and the video with stochastic outcomes when sampling. The approach is naturally implemented using a conditional invertible neural network (cINN) that can explain videos by independently modelling static and other video characteristics, thus laying the basis for controlled video synthesis. Experiments on diverse video datasets demonstrate the effectiveness of our approach in terms of both the quality and diversity of the synthesized results.
</div>




<div class="12u">
<h6>
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
Behavior-Driven Synthesis of Human Dynamics
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/behavior-driven-video-synthesis/">
<img src="https://compvis.github.io/behavior-driven-video-synthesis/images/first-page.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Generating and representing human behavior are of major importance for various computer vision applications. Commonly, human video synthesis represents behavior as sequences of postures while directly predicting their likely progressions or merely changing the appearance of the depicted persons, thus not being able to exercise control over their actual behavior during the synthesis process. In contrast, controlled behavior synthesis and transfer across individuals requires a deep understanding of body dynamics and calls for a representation of behavior that is independent of appearance and also of specific postures. In this work, we present a model for human behavior synthesis which learns a dedicated representation of human dynamics independent of postures. Using this representation, we are able to change the behavior of a person depicted in an arbitrary posture, or to even directly transfer behavior observed in a given video sequence. To this end, we propose a conditional variational framework which explicitly disentangles posture from behavior. We demonstrate the effectiveness of our approach on this novel task, evaluating capturing, transferring, and sampling fine-grained, diverse behavior, both quantitatively and qualitatively.
<!-- Insert here interactive video synthesis work from andy -->
</div>


</div>


<!-- <section id="five" class="wrapper style1"> -->
<div class="container 85%">
<div class="row 250%">
<div class="12u">
<h2> Our Related work on visual synthesis</h2>
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/taming-transformers/">
 Taming Transformers for High-Resolution Image Synthesis
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/taming-transformers/">
<img src="https://compvis.github.io/taming-transformers/paper/teaser.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers.
<!-- Insert here interactive video synthesis work from andy -->
</div>

<div class="12u">
<h6>
<a href="https://compvis.github.io/net2net/">
  Network-to-Network Translation with Conditional Invertible Neural Networks
</a>
</h6>
</div>
<div class="3u 12u$(medium)">
<div class="image fit align-center">
<a href="https://compvis.github.io/net2net/">
<img src="https://compvis.github.io/net2net/paper/teaser.png" style="max-width:25em; margin:auto" />
</a>
</div>
</div>
<div class="9u 12u$(medium)">
<p align="justify" style="line-height: 1.0em; font-size:0.9em">
Given the ever-increasing computational costs of modern machine learning models, we need to find new ways to reuse such expert models and thus tap into the resources that have been invested in their creation. Recent work suggests that the power of these massive models is captured by the representations they learn. Therefore, we seek a model that can relate between different existing representations and propose to solve this task with a conditionally invertible network. This network demonstrates its capability by (i) providing generic transfer between diverse domains, (ii) enabling controlled content synthesis by allowing modification in other domains, and (iii) facilitating diagnosis of existing representations by translating them into interpretable domains such as images. Our domain transfer network can translate between fixed representations without having to learn or finetune them. This allows users to utilize various existing domain-specific expert models from the literature that had been trained with extensive computational resources. Experiments on diverse conditional image synthesis tasks, competitive image modification results and experiments on image-to-image and text-to-image generation demonstrate the generic applicability of our approach. For example, we translate between BERT and BigGAN, state-of-the-art text and image models to provide text-to-image generation, which neither of both experts can perform on their own.
</div>
</div>
</div>
</div>
<!-- <section id="six" class="wrapper style1"> -->

</section>

<!-- Six -->
<section id="six" class="wrapper style3 special"
style="background-attachment:scroll;background-position:center bottom;">
<div class="container">
<header class="major">
<h2>Acknowledgement</h2>
<p>
The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project “KI-Absicherung – Safe AI for automated driving” and by the German Research Foundation (DFG) within project 421703927.
This page is based on a design by <a href="http://templated.co">TEMPLATED</a>.
</p>
</header>
</div>
</section>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
